# Docker Compose for Speech-to-Text Server
#
# Usage:
#   CPU Mode:
#     Start:    docker-compose up -d
#     Stop:     docker-compose down
#
#   GPU Mode (NVIDIA):
#     Start:    docker-compose --profile gpu up -d
#     Stop:     docker-compose --profile gpu down
#
#   Logs:     docker-compose logs -f
#   Rebuild:  docker-compose build --no-cache

services:
  # CPU Version (default)
  speech-to-text:
    build:
      context: ./speech-to-text-server
      dockerfile: Dockerfile
    container_name: speech-to-text-server
    profiles: ["", "cpu"]  # Default profile
    ports:
      - "5000:5000"
    environment:
      - WHISPER_MODEL=base
      - WHISPER_DEVICE=cpu
      - WHISPER_COMPUTE_TYPE=int8
    volumes:
      - whisper-cache:/home/whisper/.cache
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # GPU Version (NVIDIA CUDA)
  speech-to-text-gpu:
    build:
      context: ./speech-to-text-server
      dockerfile: Dockerfile.gpu
    container_name: speech-to-text-server
    profiles: ["gpu"]
    ports:
      - "5000:5000"
    environment:
      # Use larger model with GPU - it's fast enough
      - WHISPER_MODEL=base
      - WHISPER_DEVICE=cuda
      - WHISPER_COMPUTE_TYPE=float16
    volumes:
      - whisper-cache:/home/whisper/.cache
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

volumes:
  whisper-cache:
    name: speech-to-text-whisper-cache
