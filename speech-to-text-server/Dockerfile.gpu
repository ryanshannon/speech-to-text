# Speech-to-Text Server Dockerfile (GPU/CUDA Version)
# Uses faster-whisper with NVIDIA CUDA acceleration
#
# Requirements:
#   - NVIDIA GPU with CUDA support
#   - NVIDIA Container Toolkit installed
#   - Docker configured to use nvidia runtime

FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04

# Set working directory
WORKDIR /app

# Prevent interactive prompts during package installation
ENV DEBIAN_FRONTEND=noninteractive

# Install Python and system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.11 \
    python3.11-dev \
    python3.11-venv \
    python3-pip \
    ffmpeg \
    libsndfile1 \
    pkg-config \
    libavformat-dev \
    libavcodec-dev \
    libavdevice-dev \
    libavutil-dev \
    libswscale-dev \
    libswresample-dev \
    libavfilter-dev \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Make python3.11 the default python
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1 && \
    update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1

# Upgrade pip
RUN python -m pip install --upgrade pip

# Copy requirements first for better Docker layer caching
COPY requirements.txt .

# Install Python dependencies with CUDA support
RUN pip install --no-cache-dir -r requirements.txt

# Install cuBLAS and cuDNN for faster-whisper GPU support
RUN pip install --no-cache-dir nvidia-cublas-cu12 nvidia-cudnn-cu12

# Copy application code
COPY app.py .

# Create a non-root user for security
RUN useradd -m -u 1000 whisper && \
    chown -R whisper:whisper /app

# Create cache directory for Whisper models
RUN mkdir -p /home/whisper/.cache && \
    chown -R whisper:whisper /home/whisper/.cache

USER whisper

# Environment variables for GPU mode
ENV WHISPER_MODEL=base
ENV WHISPER_DEVICE=cuda
ENV WHISPER_COMPUTE_TYPE=float16
ENV PYTHONUNBUFFERED=1

# Expose the API port
EXPOSE 5000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \
    CMD python -c "import urllib.request; urllib.request.urlopen('http://localhost:5000/health')" || exit 1

# Run with gunicorn for production
CMD ["gunicorn", "--bind", "0.0.0.0:5000", "--workers", "1", "--timeout", "120", "app:app"]
